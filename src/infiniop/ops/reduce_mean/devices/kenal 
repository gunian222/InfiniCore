#include "reduceMean.h"
#include <cub/cub.cuh>
#include <cuda_fp16.h>

// CUDA核函数：半精度转浮点
__device__ __forceinline__ float to_float(const __half& h) {
    return __half2float(h);
}

__device__ __forceinline__ float to_float(const __nv_bfloat16& bf) {
    return __bfloat162float(bf);
}

// CUDA核函数：浮点转半精度
__device__ __forceinline__ __half to_half(float f) {
    return __float2half(f);
}

__device__ __forceinline__ __nv_bfloat16 to_bfloat16(float f) {
    return __float2bfloat16(f);
}

// 通用规约核函数
template <typename T>
__global__ void reduce_mean_kernel(
    const T* input, T* output,
    size_t outer_size, size_t dim_size, size_t inner_size
) {
    extern __shared__ float sdata[];
    
    for (size_t o = blockIdx.x; o < outer_size; o += gridDim.x) {
        for (size_t i = threadIdx.x; i < inner_size; i += blockDim.x) {
            float sum = 0.0f;
            
            for (size_t d = 0; d < dim_size; d++) {
                size_t idx = (o * dim_size * inner_size) + (d * inner_size) + i;
                sum += to_float(input[idx]);
            }
            
            size_t out_idx = o * inner_size + i;
            float avg = sum / dim_size;
            
            if (sizeof(T) == sizeof(float)) {
                output[out_idx] = avg;
            } else if (sizeof(T) == sizeof(__half)) {
                output[out_idx] = to_half(avg);
            } else if (sizeof(T) == sizeof(__nv_bfloat16)) {
                output[out_idx] = to_bfloat16(avg);
            }
        }
    }
}

infiniStatus_t reduceMean_cuda(
    infiniTensorDescriptor_t input,
    infiniTensorDescriptor_t output,
    size_t dim
) {
    // 计算维度参数
    size_t outer_size = 1;
    for (size_t i = 0; i < dim; i++) outer_size *= input->shape[i];
    
    size_t inner_size = 1;
    for (size_t i = dim + 1; i < input->shape.size(); i++) 
        inner_size *= input->shape[i];
    
    const size_t dim_size = input->shape[dim];
    
    // 获取CUDA设备属性
    cudaDeviceProp prop;
    cudaGetDeviceProperties(&prop, 0);
    
    // 配置核函数参数
    dim3 grid_size(std::min(outer_size, (size_t)prop.multiProcessorCount * 2));
    dim3 block_size(std::min(inner_size, (size_t)prop.maxThreadsPerBlock));
    size_t shared_mem = 0; // 不需要共享内存
    
    // 启动核函数
    switch(input->dtype) {
        case INFINI_DTYPE_F32:
            reduce_mean_kernel<float><<<grid_size, block_size, shared_mem>>>(
                static_cast<float*>(input->data),
                static_cast<float*>(output->data),
                outer_size, dim_size, inner_size
            );
            break;
        case INFINI_DTYPE_F16:
            reduce_mean_kernel<__half><<<grid_size, block_size, shared_mem>>>(
                static_cast<__half*>(input->data),
                static_cast<__half*>(output->data),
                outer_size, dim_size, inner_size
            );
            break;
        case INFINI_DTYPE_BF16:
            reduce_mean_kernel<__nv_bfloat16><<<grid_size, block_size, shared_mem>>>(
                static_cast<__nv_bfloat16*>(input->data),
                static_cast<__nv_bfloat16*>(output->data),
                outer_size, dim_size, inner_size
            );
            break;
        default:
            return INFINI_ERROR_UNSUPPORTED_DTYPE;
    }
    
    // 检查错误
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        return INFINI_ERROR_DEVICE;
    }
    
    // 同步设备
    cudaDeviceSynchronize();
    
    return INFINI_SUCCESS;
}